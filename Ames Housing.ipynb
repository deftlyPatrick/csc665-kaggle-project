{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Project Submission Form | Spring 2019 CSC 665 \n",
    "\n",
    "Habtom Asfaha, Antonio Carmona, Patrick Wong\n",
    "https://github.com/deftlypatrick/csc665-kaggle-project\n",
    "\n",
    "\n",
    "## 1. Dataset and Metrics\n",
    "\n",
    "1. Is it image, text processing, tabular dataset, etc.?\n",
    "\n",
    "The data set that we used is a tabular dataset containing information that describes the attributes of a number of houses that were sold in Ames, Iowa.\n",
    "\n",
    "2. What metrics does Kaggle use to evaluate this competition? Provide exact formulas, and your implementation, or refer to the existing API (NumPy, Pytorch, etc.)\n",
    "\n",
    "According to the competition’s description, prediction submissions are evaluated by calculating the root-mean-squared-error (RMSE) between the natural log applied to each predicted value and the natural log applied to each actual value. The closer that a score is to zero, the better it is. This method was claimed to be chosen over regular RMSE because taking logs results in errors in predicting the values of expensive houses affecting the score as much as errors in predicting the values of cheap houses.\n",
    "\n",
    "The log RMSE is first calculated by applying the natural log to every predicted value and every actual value. Following this, the new actual values are subtracted from the new predicted values to get error values. The error values are squared, and the mean of the squared error values is calculated. This is the MSE. Finally, the square root of this value is calculated in order to obtain the log RMSE.\n",
    "\n",
    "In order to implement this evaluation, we simply modified the parameters that our existing RMSE method in our metrics package takes in. \n",
    "\n",
    "    def mse(y_predicted, y_true):\n",
    "\n",
    "        # return mean squared error\n",
    "        return ((y_predicted - y_true) ** 2).mean()\n",
    "\n",
    "    def rmse(y_predicted, y_true):\n",
    "\n",
    "        # return root mean squared error\n",
    "        return np.sqrt(mse(y_predicted, y_true))\n",
    "\n",
    "    def rmse_log(y_predicted, y_true):\n",
    "    \n",
    "        # calculate the RMSE of the natural log applied element-wise to y_predicted and y_true\n",
    "        log_rmse = mt.rmse(np.log(y_predicted), np.log(y_true))\n",
    "    \n",
    "        return log_rmse\n",
    "\n",
    "3. How many samples does the entire dataset have?\n",
    "\n",
    "The entire dataset has 2919 samples in total, and the data set is already split into a train set and test set. There are 1460 samples in the provided train set, and 1459 samples in the provided test set (a nearly even split of the full data set).\n",
    "\n",
    "4. How many features?\n",
    "\n",
    "Each sample has 79 features. These features describe the attributes of each house (sample), with a few examples of features being lot area, house style, and the type of foundation that a house has. The value that we are attempting to predict based off of these features is the amount of money that each house will sell for. Our predictions are compared to the real observed values that each house sold for.\n",
    "\n",
    "5. What's the highest test score your achieved on Kaggle (the **actual score value**, not your leaderboard position).\n",
    "\n",
    "0.14856\n",
    "\n",
    "\n",
    "## 2. Model Selection\n",
    "\n",
    "1. What algorithms have you chosen? If you've tried multiple models, list them all. Describe your reasoning for choosing these particular models.\n",
    "\n",
    "The algorithms we tried out were Linear Regression, Decision Tree Regression, and Random Forest Regression. Because the values that we need to predict for this competition are continuous, this is a regression problem (meaning that we could rule out using any classification models). We used both our own implementations of these models and the classes provided by scikit-learn both to compare how closely they stacked up and to see which ones would do a better job of making predictions.\n",
    "\n",
    "Due to some of the features describing attributes that only a select number of samples have (e.g. pool area, because not all houses have pools), we decided to convert NaN values to zeroes instead of removing the samples containing NaN values from the set (as that would have resulted in many of the less expensive homes being removed).\n",
    "\n",
    "We ended up using Random Forest Regression in the end because the train and test scores we received with just the default settings were notably better than other methods.\n",
    "\n",
    "2. What are the best hyper-parameter settings you've found (e.g. the number of trees, any regularization, sampling ratio, learning rate, the size of the NN, etc.)\n",
    "\n",
    "In general, n_jobs was set to -1 when possible in order to use all processors to speed up fitting and predicting by running jobs in parallel for fit and predict.\n",
    "\n",
    "For Random Forest Regression, increasing the number of trees (n_estimators) up to a certain point resulted in the score improving. Many different values were tested with both the training set and test set, and it was found that the test score reaches its lowest value (~0.148) when n_estimators is around 500. Increasing or decreasing the number of estimators by increments of 100 from this point results in the score becoming worse.\n",
    "\n",
    "Increasing the minimum number of samples required for a split or the minimum number of samples required to be a leaf (min_samples_split and min_samples_leaf) from their default values of 2 and 1 both resulted in scores becoming worse as the values increased.\n",
    "\n",
    "Setting a fixed maximum depth resulted in a worse score than not setting a maximum depth, but changing the maximum value around did not result in a large amount of score variance.\n",
    "\n",
    "For the Decision Tree Regressor, increasing or decreasing the maximum depth over 100 will make the score worse and changing the minimum number of samples required to be a leafleaves that is not 2 will also make the score worse. \n",
    "\n",
    "This suggests that the best decision tree is reached at the max_depth of 100 and the min_sample_leaves at 2 which is the most standard approach to the dataset. \n",
    "\n",
    "\n",
    "## 3. Training & Validation\n",
    "\n",
    "1. What is your training and test split approach?\n",
    "\n",
    "Due to the format of the dataset (the training set and test set were provided to us pre-split), we were unable to manually shuffle and split the full data set. The training and test sets we were given were split nearly 1:1 (the test set has one less sample than the training set) and do not appear to be sorted by any of the samples’ features.\n",
    "\n",
    "If we did have to split the training and test sets manually, we would have done so by shuffling the full data set and then splitting with a ratio such as 2:1 (66% training set, 33% test set) in order to avoid having our training set sorted in any way and to have a good variety of samples to train on (since having more samples in our training set would reduce the chances of something unexpected for our model showing up in the test set).\n",
    "\n",
    "2. What methods did you use to evaluate your performance on your datasets. Provide the exact formula and the implementation, or refer to an existing API.\n",
    "\n",
    "To evaluate the performance of our dataset, we used both RMSE with the logarithms of values (which is the formula used by Kaggle to evaluate this competition) and RSQ. With the RMSE of logarithms, the goal is a score closest to zero. With RSQ, values closest to one are best. \n",
    "\n",
    "    def mse(y_predicted, y_true):\n",
    "\n",
    "        # return mean squared error\n",
    "        return ((y_predicted - y_true) ** 2).mean()\n",
    "\n",
    "    def rmse(y_predicted, y_true):\n",
    "\n",
    "        # return root mean squared error\n",
    "        return np.sqrt(mse(y_predicted, y_true))\n",
    "\n",
    "    def rmse_log(y_predicted, y_true):\n",
    "    \n",
    "        # calculate the RMSE of the natural log applied element-wise to y_predicted and y_true\n",
    "        log_rmse = mt.rmse(np.log(y_predicted), np.log(y_true))\n",
    "    \n",
    "        return log_rmse\n",
    "        \n",
    "    def rsq(y_predicted, y_true):\n",
    "\n",
    "        # return score\n",
    "        v = ((y_true - y_true.mean()) ** 2).mean()\n",
    "        return 1 - mse(y_predicted, y_true) / v\n",
    "\n",
    "   2.1 What is the best score you've achieved?   \n",
    "   \n",
    "Our best train scores are as follows.\n",
    "\n",
    "    Log RMSE: 0.05841479022158461\n",
    "    RSQ: 0.9820542250900609\n",
    "\n",
    "## 4. Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Download Link: https://www.kaggle.com/c/5407/download-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csc665.features as ft\n",
    "import csc665.metrics as mt\n",
    "from csc665.ensemble import RandomForestRegressor as our_rfr\n",
    "from sklearn.ensemble import RandomForestRegressor as sk_rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(csv_df, target_col_name):\n",
    "    \n",
    "    # replace all NA values with zero\n",
    "    csv_df_temp = csv_df.copy().fillna(0)\n",
    "    \n",
    "    # convert all strings to numbers\n",
    "    string_columns = list(csv_df_temp.select_dtypes(exclude='number'))\n",
    "    ft.create_categories(csv_df_temp, string_columns)\n",
    "    \n",
    "    # split the data frame into x and y\n",
    "    csv_df_x = csv_df_temp.drop(target_col_name, axis=1)\n",
    "    csv_df_y = csv_df_temp[target_col_name].values\n",
    "    \n",
    "    return csv_df_x, csv_df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(csv_df):\n",
    "    \n",
    "    # replace all NA values with zero\n",
    "    csv_df_temp = csv_df.copy().fillna(0)\n",
    "    \n",
    "    # convert all strings to numbers\n",
    "    string_columns = list(csv_df_temp.select_dtypes(exclude='number'))\n",
    "    ft.create_categories(csv_df_temp, string_columns)\n",
    "    \n",
    "    return csv_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_log(y_predicted, y_true):\n",
    "    \n",
    "    # calculate the RMSE of the natural log applied element-wise to y_predicted and y_true\n",
    "    log_rmse = mt.rmse(np.log(y_predicted), np.log(y_true))\n",
    "    \n",
    "    return log_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/train.csv' does not exist: b'data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-b445c2f7703c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read CSV files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/train.csv' does not exist: b'data/train.csv'"
     ]
    }
   ],
   "source": [
    "# read CSV files\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and split train set into x and y, replace all NaN elements with 0\n",
    "x_train_df, y_train_df = preprocess_train(train_df, \"SalePrice\")\n",
    "\n",
    "# convert all strings in test set to numbers, replace all NaN elements with 0\n",
    "x_test_df = preprocess_test(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Random Forest Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit our random forest regressor\n",
    "ours = our_rfr(10, 0.4)\n",
    "ours.fit(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log RMSE:  0.11568219492658534\n",
      "RSQ:  0.9251546917272753\n"
     ]
    }
   ],
   "source": [
    "# predict and score (train set)\n",
    "ours_prediction_train = ours.predict(x_train_df)\n",
    "ours_score_train = mt.rsq(ours_prediction_train, y_train_df)\n",
    "ours_rmse_log_train = rmse_log(ours_prediction_train, y_train_df)\n",
    "print(\"Log RMSE: \", ours_rmse_log_train)\n",
    "print(\"RSQ: \", ours_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict (test set)\n",
    "ours_prediction_test = ours.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn's Random Forest Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit scikit-learn's random forest regressor\n",
    "sk = sk_rfr(n_estimators=500, n_jobs=-1)\n",
    "sk.fit(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log RMSE:  0.058585506052791565\n",
      "RSQ:  0.9818653022577345\n"
     ]
    }
   ],
   "source": [
    "# predict and score (train set)\n",
    "sk_prediction_train = sk.predict(x_train_df)\n",
    "sk_score_train = sk.score(x_train_df, y_train_df)\n",
    "sk_rmse_log_train = rmse_log(sk_prediction_train, y_train_df)\n",
    "print(\"Log RMSE: \", sk_rmse_log_train)\n",
    "print(\"RSQ: \", sk_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict (test set)\n",
    "sk_prediction_test = sk.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output predictions from both our random forest regressor and scikit-learn's random forest regressor\n",
    "sk_predictions = pd.DataFrame({'Id': x_test_df.Id, 'SalePrice': sk_prediction_test})\n",
    "sk_predictions.to_csv('submission_sk.csv', index=False)\n",
    "our_predictions = pd.DataFrame({'Id': x_test_df.Id, 'SalePrice': ours_prediction_test})\n",
    "our_predictions.to_csv('submission_665.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORE\n",
      "Log RMSE:  0.058585506052791565\n",
      "RSQ:  0.9251546917272753\n",
      "TEST SCORE\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"TRAIN SCORE\")\n",
    "# Log RMSE\n",
    "if sk_rmse_log_train < ours_rmse_log_train:\n",
    "    print(\"Log RMSE: \", sk_rmse_log_train)\n",
    "else:\n",
    "    print(\"Log RMSE: \", ours_rmse_log_train)\n",
    "\n",
    "# RSQ\n",
    "if sk_score_train < ours_score_train:\n",
    "    print(\"RSQ: \", sk_score_train)\n",
    "else:\n",
    "    print(\"RSQ: \", ours_score_train)\n",
    "    \n",
    "print(\"TEST SCORE\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='test_score.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
